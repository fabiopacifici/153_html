<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Attention is All you need</title>
</head>

<body>
  <!-- Headear -->
  <header>
    <h1>Attention is All You Need</h1>

    <hr>
  </header>
  <!-- /Headear -->

  <!-- Main -->
  <main>

    <div class="autor">
      <strong>Ashish Vaswani</strong><br><br>
      <span>Google Brain</span><br><br>
      <em>avaswani@google.com</em>

    </div>
    <!-- /.author -->

    <section class="abstract">
      <h2>Abstract</h2>
      <p>The dominant sequence transduction models are based on complex recurrent or
        convolutional neural networks that include an encoder and a decoder. The best
        performing models also connect the encoder and decoder through an attention
        mechanism. We propose a new simple network architecture, the Transformer,
        based solely on attention mechanisms, dispensing with recurrence and convolutions
        entirely. Experiments on two machine translation tasks show these models to
        be superior in quality while being more parallelizable and requiring significantly
        less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving
        over
        the
        existing best results, including
        ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
        our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
        training for 3.5 days on eight GPUs, a small fraction of the training costs of the
        best models from the literature. We show that the Transformer generalizes well to
        other tasks by applying it successfully to English constituency parsing both with
        large and limited training data.
      </p>


      <ul>
        <li>this is a list item</li>
        <li>this is a list item</li>
        <li>this is a list item</li>
        <strong>this a strong tag inside an UL ðŸ˜±</strong>
        <p>this a p tag inside an UL ðŸ˜±</p>
      </ul>


      <ol>
        <li>
          <strong>Introduction</strong>
          <p>
            Lorem ipsum dolor sit amet, consectetur adipisicing elit. Voluptatum modi voluptas ea adipisci unde id,
            quidem
            quam architecto corrupti, reiciendis, vero porro enim error iure deserunt optio dignissimos consectetur
            doloribus!
          </p>
          <figure>
            <!--   <img width="200" src="https://arxiv.org/html/1706.03762v7/Figures/ModalNet-21.png"
              alt="self attention diagram"> -->
            <img width="200" src="./img/ModalNet-21.png" alt="self attention diagram">
          </figure>
        </li>

      </ol>

    </section>
    <!-- /.abstract -->


  </main>
  <!-- /Main -->

  <footer>
    <!-- This link shows the external website in the same browser tab 
    <a href="https://arxiv.org/abs/1706.03762">
      Go to the paper
    </a>-->

    <!-- This link uses the target _blank that opens a new browser tab -->
    <a href="https://arxiv.org/abs/1706.03762">
      Go to the paper
    </a>
  </footer>



</body>

</html>